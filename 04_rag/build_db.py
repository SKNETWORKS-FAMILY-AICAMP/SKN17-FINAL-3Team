import os
import re
import glob
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ==========================================
# [ì„¤ì •] API í‚¤ ì…ë ¥ (ìƒˆë¡œ ë°œê¸‰ë°›ì€ í‚¤ ì‚¬ìš©)
# ==========================================
os.environ["OPENAI_API_KEY"] = ""

DATA_DIR = "./"
DB_PATH = "./faiss_index"

def parse_match_log_clean(file_path: str):
    """
    ì œëª©ì—ì„œ (ë‚ ì§œ) ë¶€ë¶„ì„ ì•„ì˜ˆ ì œê±°í•˜ê³  ìˆœìˆ˜ ì œëª©ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥
    """
    print(f"   Processing: {os.path.basename(file_path)}")
    
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        text = f.read()

    # '---' êµ¬ë¶„ìë¡œ ê²½ê¸° ë¶„ë¦¬
    raw_matches = text.split('\n---')
    docs = []

    for match_text in raw_matches:
        match_text = match_text.strip()
        if not match_text:
            continue

        # -------------------------------------------------
        # [í•µì‹¬ ìˆ˜ì •] ì œëª© ì •ì œ ë¡œì§ (ë‚ ì§œ ì‚­ì œ)
        # -------------------------------------------------
        lines = match_text.split('\n')
        clean_title = "Unknown Match"
        
        for line in lines:
            # '#'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í—¤ë” ë¼ì¸ ì°¾ê¸°
            if line.strip().startswith("#"):
                # 1. '#' ì œê±°
                temp_title = line.replace("#", "").strip()
                
                # 2. ( ) ê´„í˜¸ì™€ ê·¸ ì•ˆì˜ ë‚´ìš©(ë‚ ì§œ)ì„ ì •ê·œì‹ìœ¼ë¡œ ì‚­ì œ
                # ì˜ˆ: "2025 í•œêµ­ì‹œë¦¬ì¦ˆ (2025-10-26)" -> "2025 í•œêµ­ì‹œë¦¬ì¦ˆ"
                temp_title = re.sub(r"\(.*?\)", "", temp_title)
                
                # 3. ì•ë’¤ ê³µë°± í•œë²ˆ ë” ì œê±°
                clean_title = temp_title.strip()
                break
        
        # ë¡œê·¸ë¡œ í™•ì¸
        print(f"    >> ì œëª© ì¶”ì¶œ: '{clean_title}'") 

        metadata = {
            "source": file_path,
            "type": "match_log",
            "match_title": clean_title,
        }

        docs.append(Document(page_content=match_text, metadata=metadata))
        
    return docs

def main():
    all_documents = []
    md_files = glob.glob(os.path.join(DATA_DIR, "*.md"))
    
    print(f"ğŸ“‚ íŒŒì¼ ëª©ë¡: {md_files}")

    if not md_files:
        print("âŒ ì˜¤ë¥˜: .md íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1. íŒŒì‹± (Parsing)
    for file_path in md_files:
        filename = os.path.basename(file_path)
        
        # ê·œì •ì§‘ì´ ì•„ë‹Œ ê²½ìš° -> ê²½ê¸° ê¸°ë¡
        if "ê·œì •" not in filename and "ê·œì¹™" not in filename:
            all_documents.extend(parse_match_log_clean(file_path))
        else:
            # ê·œì •ì§‘ì€ í†µì§¸ë¡œ
            with open(file_path, 'r', encoding='utf-8') as f:
                all_documents.append(Document(page_content=f.read(), metadata={"match_title": "ê·œì •ì§‘"}))

    if not all_documents:
        print("âŒ ìƒì„±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“Š ì´ {len(all_documents)}ê°œì˜ ë¬¸ì„œ ë¡œë“œë¨.")

    # 2. ì²­í‚¹ (Chunking)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_docs = text_splitter.split_documents(all_documents)

    # 3. ì„ë² ë”© ë° ì €ì¥
    print("ğŸš€ ë²¡í„° DB ìƒì„± ì‹œì‘...")
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    vectorstore.save_local(DB_PATH)
    print(f"âœ… DB ìƒì„± ì™„ë£Œ! ê²½ë¡œ: {DB_PATH}")

if __name__ == "__main__":
    main()